{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SimpleCorridor.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install gym\n",
        "!pip install tensorflow\n",
        "!pip install -U \"ray[rllib]\""
      ],
      "metadata": {
        "id": "ZKym35jFKfz5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gym, ray\n",
        "import numpy as np\n",
        "from ray.rllib.agents import dqn"
      ],
      "metadata": {
        "id": "w6eLqYYxQJmc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Create an env\n",
        "- connect this env to the DQN agent (implemented in Ray)\n",
        "- Train the agent for the env"
      ],
      "metadata": {
        "id": "C_hq_mUkQU5X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# |0|0|0|0|0|1|\n",
        "#  X\n",
        "#  ---------->\n",
        "# negative reward for not reaching the terminal state -0.1 / step\n",
        "\n",
        "\n",
        "# gym.Env: class defining a standard env\n",
        "\n",
        "\n",
        "class SimpleCorridor(gym.Env):\n",
        "  def __init__(self, config):\n",
        "    # the position of an agent\n",
        "    self.end_pos = 5 # term at cell 5\n",
        "    self.cur_pos = 0 # init with cell 0\n",
        "\n",
        "    # define action/state space\n",
        "    self.action_space = gym.spaces.Discrete(2) # left/right -> 2 disc actions\n",
        "    self.observation_space = gym.spaces.Discrete(self.end_pos + 1)\n",
        "  \n",
        "  def reset(self):\n",
        "    self.cur_pos = 0 # init loc\n",
        "    return self.cur_pos\n",
        "\n",
        "  def step(self, action):\n",
        "    # update the cur_pos based on the action\n",
        "    # aciton Discrete(2) -> action 0 or action 1\n",
        "    # right 0, left 1\n",
        "    if action == 0 and self.cur_pos < self.end_pos:\n",
        "      self.cur_pos += 1\n",
        "    elif action == 1 and self.cur_pos > 0:\n",
        "      self.cur_pos -= 1\n",
        "\n",
        "    pseudo_info = {}\n",
        "    if self.cur_pos >= self.end_pos:\n",
        "      return 0, 1.0, True, pseudo_info\n",
        "    else:\n",
        "      return self.cur_pos, -0.1, False, pseudo_info"
      ],
      "metadata": {
        "id": "sWzhf4hOQQgP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# conenct this to ray DQN\n",
        "\n",
        "ray.shutdown()\n",
        "ray.init() # start ray training env\n",
        "\n",
        "config = {\n",
        "    \"env\": SimpleCorridor,\n",
        "    \"env_config\": {\n",
        "        \"gamma\": 0.8\n",
        "    },\n",
        "}\n",
        "\n",
        "# prepare an agent\n",
        "trainer = dqn.DQNTrainer(config=config)\n",
        "\n",
        "# train the agent with 2 batches\n",
        "for _ in range(5):\n",
        "  print(trainer.train())\n",
        "\n",
        "trainer.evaluate()"
      ],
      "metadata": {
        "id": "qehoXdQuWFr4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}